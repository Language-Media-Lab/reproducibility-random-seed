{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3FxryDHCG8r",
    "outputId": "f3c19fea-b755-47b9-f673-27dded181b32"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.12.5 fugashi==1.1.0 ipadic==1.0.0 sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOz2bkuCCKzi"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig,AdamW,T5Tokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed = 0\n",
    "fix_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoqILLv4FgeM"
   },
   "outputs": [],
   "source": [
    "#データのダウンロード\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n",
    "#ファイルの解凍\n",
    "!tar -zxf ldcc-20140209.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgux6EdvFieK"
   },
   "outputs": [],
   "source": [
    "!cat ./text/it-life-hack/it-life-hack-6342280.txt # ファイルを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oN8-NI3RG-H0"
   },
   "outputs": [],
   "source": [
    "category_list = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbfKJvTLcs--"
   },
   "outputs": [],
   "source": [
    "def train(model,loader):\n",
    "    model.train() \n",
    "    train_loss = 0\n",
    "    for batch in loader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids,\n",
    "                      attention_mask=b_input_mask, \n",
    "                      labels=b_labels)\n",
    "        loss = output[0]\n",
    "        loss = loss.to(torch.float32)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validation(model,loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch in loader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "            output = model(b_input_ids, \n",
    "                          attention_mask=b_input_mask,\n",
    "                          labels=b_labels)\n",
    "            val_loss += output[0]\n",
    "    return val_loss\n",
    "\n",
    "def test_report(model,loader):\n",
    "    pred_result = []\n",
    "    true_result = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            pred_labels = [np.argmax(pred.to('cpu').detach().numpy()) for pred in output[1]]\n",
    "            pred_result.extend(pred_labels)\n",
    "            true_result.extend(batch['labels'])\n",
    "    return classification_report(true_result,pred_result,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhzdG5YtMw-W"
   },
   "outputs": [],
   "source": [
    "# 参照 https://qiita.com/ku_a_i/items/ba33c9ce3449da23b503\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"earlystoppingクラス\"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, verbose=False, path='checkpoint_model.pth', threshold=10000):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience    #設定ストップカウンタ\n",
    "        self.verbose = verbose      #表示の有無\n",
    "        self.counter = 0            #現在のカウンタ値\n",
    "        self.best_score = None      #ベストスコア\n",
    "        self.early_stop = False     #ストップフラグ\n",
    "        self.val_loss_min = np.Inf   #前回のベストスコア記憶用\n",
    "        self.path = path             #ベストモデル格納path\n",
    "        self.tloss_th = threshold   #学習ロスの閾値（指定しない場合は10000になり，事実上閾値なし）\n",
    "\n",
    "    def __call__(self, train_loss, val_loss, model):\n",
    "        \"\"\"\n",
    "        特殊(call)メソッド\n",
    "        実際に学習ループ内で最小lossを更新したか否かを計算させる部分\n",
    "        \"\"\"\n",
    "        score = -val_loss\n",
    "\n",
    "\n",
    "        if self.best_score is None or train_loss>self.tloss_th:  #1Epoch目または閾値以上での処理\n",
    "            self.best_score = score   #1Epoch目はそのままベストスコアとして記録する\n",
    "            self.checkpoint(val_loss, model)  #記録後にモデルを保存してスコア表示する\n",
    "        elif score < self.best_score:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1   #ストップカウンタを+1\n",
    "            if self.verbose:  #表示を有効にした場合は経過を表示\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')  #現在のカウンタを表示する \n",
    "            if self.counter >= self.patience:  #設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "        else:  #ベストスコアを更新した場合\n",
    "            self.best_score = score  #ベストスコアを上書き\n",
    "            self.checkpoint(val_loss, model)  #モデルを保存してスコア表示\n",
    "            self.counter = 0  #ストップカウンタリセット\n",
    "\n",
    "    def checkpoint(self, val_loss, model):\n",
    "        '''ベストスコア更新時に実行されるチェックポイント関数'''\n",
    "        if self.verbose:  #表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)  #ベストモデルを指定したpathに保存\n",
    "        self.val_loss_min = val_loss  #その時のlossを記録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap6pgP97PXN6"
   },
   "outputs": [],
   "source": [
    "def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "    \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "        expects epoch to start from 1.\n",
    "    \"\"\"\n",
    "    x = range(1, epoch+1)\n",
    "    y = np.concatenate((train_loss, valid_loss))\n",
    "    graphs = [[x,train_loss], [x,valid_loss]]\n",
    "    x_margin = 0.2\n",
    "    y_margin = 0.05\n",
    "    x_bounds = [1-x_margin, epochs+x_margin]\n",
    "    y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "    mb.update_graph(graphs, x_bounds, y_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4jcc28nPLYF"
   },
   "source": [
    "# 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGxYDEm5ElcJ"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"bandainamco-mirai/distilbert-base-japanese\"\n",
    "# MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "# MODEL_NAME = \"rinna/japanese-roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMA8yA-BEpVn"
   },
   "source": [
    "## トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wq0Xtk2DbRKo"
   },
   "outputs": [],
   "source": [
    "# トークナイザのロード\n",
    "if MODEL_NAME ==\"bandainamco-mirai/distilbert-base-japanese\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "elif MODEL_NAME== \"rinna/japanese-roberta-base\":\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 各データの形式を整える（参照：https://github.com/stockmarkteam/bert-book/blob/master/Chapter6.ipynb）\n",
    "max_length = 256\n",
    "dataset_for_loader = []\n",
    "for label, category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
    "        lines = open(file).read().splitlines()\n",
    "        text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す。\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        encoding['labels'] = label # ラベルを追加\n",
    "        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMOOvhLOkyy8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"livedoor_\"+MODEL_NAME.replace(\"/\",\"_\")+\"_tokenized.pkl\",\"wb\") as f:\n",
    "#   pickle.dump(dataset_for_loader,f)\n",
    "\n",
    "with open(\"livedoor_\"+MODEL_NAME.replace(\"/\",\"_\")+\"_tokenized.pkl\",\"rb\") as f:\n",
    "    dataset_for_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yQRYABNJIX1"
   },
   "outputs": [],
   "source": [
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMXgq05AOkAp"
   },
   "source": [
    "## データセットの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0dqekmLKoIo"
   },
   "outputs": [],
   "source": [
    "# データセットの分割\n",
    "fix_seed(0)\n",
    "random.shuffle(dataset_for_loader) # ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)+n_train\n",
    "dataset_train = dataset_for_loader[:n_train] # 学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_val] # 検証データ\n",
    "dataset_test = dataset_for_loader[n_val:] # テストデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrcABsQ4OnAa"
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBoYuhcSLB03"
   },
   "outputs": [],
   "source": [
    "all_results={}\n",
    "\n",
    "for n in trange(25):\n",
    "    # print('===== '+str(n)+\" =====\")\n",
    "    fix_seed(n)\n",
    "\n",
    "    earlystopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "    dataloader_train = DataLoader(\n",
    "      dataset_train, batch_size=32, shuffle=True\n",
    "    ) \n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=256)\n",
    "\n",
    "    # モデルの読み込み\n",
    "    bert_sc = AutoModelForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=9,\n",
    "            output_attentions = False,\n",
    "            output_hidden_states = False,\n",
    "            )\n",
    "    bert_sc.cuda()\n",
    "\n",
    "    optimizer = AdamW(bert_sc.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(30):\n",
    "        # print(\"epoch: \"+str(epoch+1))\n",
    "        train_loss = train(bert_sc,dataloader_train)\n",
    "        val_loss = validation(bert_sc,dataloader_val)\n",
    "        print(\"train loss: \"+str(train_loss))\n",
    "        # print(\"val loss: \"+str(val_loss))\n",
    "\n",
    "    earlystopping(val_loss, bert_sc) #callメソッド呼び出し\n",
    "    if earlystopping.early_stop: #ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "        print(\"Early Stopping!\")\n",
    "        break\n",
    "    bert_sc.load_state_dict(torch.load('checkpoint_model.pth'))\n",
    "    print(\"loaded best model\")\n",
    "    all_results[n]=test_report(bert_sc,dataloader_test)\n",
    "\n",
    "\n",
    "with open(\"testset_\"+MODEL_NAME.replace(\"/\",\"_\")+\"_livedoor.pkl\",\"wb\") as f:\n",
    "    pickle.dump(all_results,f)\n",
    "    print(\"saved tes_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p85Ee-iPrVdW"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"testset_\"+MODEL_NAME.replace(\"/\",\"_\")+\"_livedoor.pkl\",\"wb\") as f:\n",
    "#   pickle.dump(all_results,f)\n",
    "#   print(\"saved tes_list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQxiaG4qOqG5"
   },
   "source": [
    "## スコア分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbAEA54rKsTC"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"testset_\"+MODEL_NAME.replace(\"/\",\"_\")+\"_livedoor.pkl\",\"rb\")as f:\n",
    "    l = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRwtDk33LAI5"
   },
   "outputs": [],
   "source": [
    "acc_lis = [result['accuracy'] for result in l.values()]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jbMge2Ydiau"
   },
   "outputs": [],
   "source": [
    "sns.histplot(acc_lis, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXaRNGFGeTRD"
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfCJeX3Deld3"
   },
   "outputs": [],
   "source": [
    "def interval_estimate(lis,alpha=0.95):\n",
    "    import numpy as np\n",
    "    lis = np.array(lis)\n",
    "    t_dist = stats.t(loc=lis.mean(),scale=np.sqrt(lis.var()/len(lis)),df=len(lis))\n",
    "    return t_dist.interval(alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_m2Owy01e5Mc",
    "outputId": "f7ea9fc5-8c26-467b-ffa2-05278fd068d8"
   },
   "outputs": [],
   "source": [
    "interval_estimate(acc_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71dsk6B0kW52"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "a4jcc28nPLYF",
    "VBgU4LwZO2Rx",
    "TNjbAalbPYzl"
   ],
   "name": "RS_JP_livdoor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
